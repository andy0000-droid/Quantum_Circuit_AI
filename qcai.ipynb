{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discord_notify import send_discord_notification\n",
    "from gemini import get_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_discord_notification(\"Start Quantum Circuit Artificail Intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = get_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_logger(original_function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        process_start_time = time.process_time()\n",
    "        performance_start_time = time.perf_counter()\n",
    "\n",
    "        result = original_function(*args, **kwargs)\n",
    "        \n",
    "        process_end_time = time.process_time()\n",
    "        performance_end_time = time.perf_counter()\n",
    "        execution_time = process_end_time - process_start_time\n",
    "        performance_time = performance_end_time - performance_start_time\n",
    "        print(f\"Execution time: {execution_time:.6f} seconds\", end=\", \")\n",
    "        print(f\"Performance time: {performance_time:.6f} seconds\")\n",
    "        send_discord_notification(f\"Execution time: {execution_time:.6f} seconds, Performance time: {performance_time:.6f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update_Model_List(_model_name, _prompt, _model, _chain_list):\n",
    "    print(f\"Updating model list with model: {_model_name}\")\n",
    "    global models_list\n",
    "    if _model_name not in models_list:\n",
    "        models_list.append(_model_name)\n",
    "    _temp = dict()\n",
    "    _temp[_model_name] = _prompt | _model\n",
    "    _chain_list.append(_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def Get_Answer_caller(_chain_list, query):\n",
    "    _answer_list = list()\n",
    "    for models in _chain_list:\n",
    "        for name, model in models.items():\n",
    "            print(name, end=\": \")\n",
    "            process_start_time = time.process_time()\n",
    "            performance_start_time = time.perf_counter()\n",
    "\n",
    "            _answer_list.append({name : model.invoke({\"question\": query})})\n",
    "\n",
    "            process_end_time = time.process_time()\n",
    "            performance_end_time = time.perf_counter()\n",
    "            execution_time = process_end_time - process_start_time\n",
    "            performance_time = performance_end_time - performance_start_time\n",
    "            print(f\"Execution time: {execution_time:.4f} seconds\", end=\", \")\n",
    "            print(f\"Performance time: {performance_time:.4f} seconds\")\n",
    "    return _answer_list\n",
    "\"\"\"\n",
    "\n",
    "def Get_Answer_caller(query):\n",
    "    _query = query\n",
    "    @time_logger\n",
    "    def Get_Answer(model):\n",
    "        return model.invoke({\"question\": _query})\n",
    "    return Get_Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating model list with model: deepseek-r1:70b\n",
      "Updating model list with model: gemma3:27b\n",
      "Updating model list with model: llama3.3:70b\n",
      "Updating model list with model: llama3.1:70b\n",
      "Updating model list with model: Gemini-2.5-pro\n",
      "Models List:\n",
      "deepseek-r1:70b, gemma3:27b, llama3.3:70b, llama3.1:70b, Gemini-2.5-pro\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import ollama\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "chain_list = list()\n",
    "model_dict = dict()\n",
    "models_list = list()\n",
    "for _ in ollama.list().models:\n",
    "    models_list.append(_[\"model\"])\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Answer the question in Korean.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "for model_name in models_list:\n",
    "    model = OllamaLLM(model=model_name)\n",
    "    Update_Model_List(model_name, prompt, model, chain_list)\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.5-pro-exp-03-25\", google_api_key=GEMINI_API_KEY)\n",
    "Update_Model_List(\"Gemini-2.5-pro\", prompt, llm, chain_list)\n",
    "\n",
    "print(\"Models List:\")\n",
    "print(\", \".join(models_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek-r1:70b: Execution time: 0.073593 seconds, Performance time: 31.692717 seconds\n",
      "gemma3:27b: Execution time: 0.010257 seconds, Performance time: 6.987489 seconds\n",
      "llama3.3:70b: Execution time: 0.012517 seconds, Performance time: 10.342857 seconds\n",
      "llama3.1:70b: Execution time: 0.032485 seconds, Performance time: 28.982913 seconds\n",
      "Gemini-2.5-pro: Execution time: 0.016200 seconds, Performance time: 2.386745 seconds\n"
     ]
    }
   ],
   "source": [
    "answer_list = list()\n",
    "answer_caller = Get_Answer_caller(\"What is the capital of South Korea?\")\n",
    "for models in chain_list:\n",
    "    for name, model in models.items():\n",
    "        print(name, end=\": \")\n",
    "        send_discord_notification(f\"Start {name}\")\n",
    "        answer = answer_caller(model)\n",
    "        send_discord_notification(f\"End {name}\")\n",
    "        answer_list.append({name: answer})\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qcai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
