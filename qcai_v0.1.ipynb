{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from discord_notify import send_discord_notification\n",
    "from gemini import get_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "send_discord_notification(\"Start Quantum Circuit Artificail Intelligence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_API_KEY = get_api_key()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_logger(original_function):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        import time\n",
    "        process_start_time = time.process_time()\n",
    "        performance_start_time = time.perf_counter()\n",
    "        result = original_function(*args, **kwargs)\n",
    "        process_end_time = time.process_time()\n",
    "        performance_end_time = time.perf_counter()\n",
    "        execution_time = process_end_time - process_start_time\n",
    "        performance_time = performance_end_time - performance_start_time\n",
    "        print(f\"Execution time: {execution_time:.4f} seconds\")\n",
    "        print(f\"Performance time: {performance_time:.4f} seconds\")\n",
    "        return result\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Update_Model_List(_model_name, _prompt, _model, _chain_list):\n",
    "    print(f\"Updating model list with model: {_model_name}\")\n",
    "    global models_list\n",
    "    if _model_name not in models_list:\n",
    "        models_list.append(_model_name)\n",
    "    _temp = dict()\n",
    "    _temp[_model_name] = _prompt | _model\n",
    "    _chain_list.append(_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Answer(_chain_list, query):\n",
    "    _answer_list = list()\n",
    "    for models in _chain_list:\n",
    "        for name, model in models.items():\n",
    "            print(name)\n",
    "            _answer_list.append({name : model.invoke({\"question\": query})})\n",
    "    return _answer_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import ollama\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "\n",
    "chain_list = list()\n",
    "model_dict = dict()\n",
    "models_list = list()\n",
    "for _ in ollama.list().models:\n",
    "    models_list.append(_[\"model\"])\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step. Answer the question in Korean.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "for model_name in models_list:\n",
    "    model = OllamaLLM(model=model_name)\n",
    "    Update_Model_List(model_name, prompt, model, chain_list)\n",
    "\n",
    "llm = GoogleGenerativeAI(model=\"gemini-2.5-pro-exp-03-25\", google_api_key=GEMINI_API_KEY)\n",
    "Update_Model_List(\"Gemini-2.5-pro\", prompt, llm, chain_list)\n",
    "\n",
    "print(\"Models List:\")\n",
    "print(\", \".join(models_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Get_Answer(chain_list, \"What is the capital of South Korea?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qcai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
